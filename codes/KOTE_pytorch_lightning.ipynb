{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch ## version >= 1.8.2\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl ## version == 1.4.9\n",
    "\n",
    "import datasets ## version == 2.1.0\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel ## version == 4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: kote/dichotomized\n",
      "Reusing dataset kote (/home/ubuntu/.cache/huggingface/datasets/searle-j___kote/dichotomized/0.0.0/9e18d6e4c5fb5b54c412810da99dfa5e5ece83c40924ee5eb3f41ce5b4d5b436)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c882d3b8434c448b4466b32e578b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"searle-j/kote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'text', 'labels'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'text', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'text', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the shape.\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '32521',\n",
       " 'text': '구슬픈 봄날 저녁 무렵, 도시의 뒤섞여 있는 건축과 건축의 그림자를 찾아서 커다란 군중 속에 휩쓸려 가는 것은 얼마나 즐거운 일인가. <우울한 고양이_하기와라 사쿠타로>15',\n",
       " 'labels': [2, 4, 5, 13, 14, 15, 16, 27, 28, 38, 40, 42]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check one sample in the train set.\n",
    "\n",
    "dataset[\"train\"][25597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels shape ::: (40000, 44)\n",
      "test_labels shape :::: (5000, 44)\n",
      "val_labels shape ::::: (5000, 44)\n",
      "\n",
      "cool..!!\n"
     ]
    }
   ],
   "source": [
    "## convert the integer labels into multi-hot form (44-dimensional).\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(dataset[\"train\"][\"labels\"])\n",
    "test_labels = mlb.fit_transform(dataset[\"test\"][\"labels\"])\n",
    "val_labels = mlb.fit_transform(dataset[\"validation\"][\"labels\"])\n",
    "\n",
    "print(\"train_labels shape ::: {}\".format(train_labels.shape))\n",
    "print(\"test_labels shape :::: {}\".format(test_labels.shape))\n",
    "print(\"val_labels shape ::::: {}\".format(val_labels.shape))\n",
    "print(\"\\ncool..!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract the texts, since we will use a custom datset not the huggingface dataset.\n",
    "\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "test_texts = dataset[\"test\"][\"text\"]\n",
    "val_texts = dataset[\"validation\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## and the label names...\n",
    "\n",
    "LABELS = dataset[\"train\"].features[\"labels\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download the pretrained tokenizer from huggingface.\n",
    "\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base\" # <-- Thank you!\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let us mask and switch some tokens in the train set for a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_masking(encoding, prob):\n",
    "    for i, token in enumerate(encoding[\"input_ids\"][0]):\n",
    "        if token not in [0,1,2,3]: # 0 ~ 3, [PAD], [UNK], [CLS], and [SEP], respectively.\n",
    "            if np.random.uniform(0,1) < prob:\n",
    "                encoding[\"input_ids\"][0][i] = 4 # 4 is [MASK]\n",
    "                \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_switching(encoding, prob):\n",
    "    for i, token in enumerate(encoding[\"input_ids\"][0]):\n",
    "        if token not in [0,1,2,3,4]: # 0 ~ 4, [PAD], [UNK], [CLS], [SEP], and [MASK], respectively.\n",
    "            if np.random.uniform(0,1) < prob:\n",
    "                encoding[\"input_ids\"][0][i] = np.random.choice(np.arange(5,tokenizer.vocab_size), 1)[0]\n",
    "                \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_switch(encoding, prob=0.1):\n",
    "    encoding = token_masking(encoding, prob/2)\n",
    "    encoding = token_switching(encoding, prob/2)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maximum token lengths\n",
    "\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define our dataset...!\n",
    "\n",
    "class KOTEDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length:int=MAX_LENGTH,\n",
    "                would_you_like_some_mask_and_switch:bool=False):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask = would_you_like_some_mask_and_switch\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        \n",
    "        if self.mask:\n",
    "            encoding = mask_and_switch(encoding, prob=0.1)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return dict(\n",
    "          input_ids=encoding[\"input_ids\"].flatten(),\n",
    "          attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "          labels=torch.FloatTensor(labels), ## must be a float tensor.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the datasets.\n",
    "\n",
    "train_dataset = KOTEDataset(train_texts, train_labels, tokenizer=tokenizer, would_you_like_some_mask_and_switch=True)\n",
    "test_dataset = KOTEDataset(test_texts, test_labels, tokenizer=tokenizer)\n",
    "val_dataset = KOTEDataset(val_texts, val_labels, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## download the pretrained electra model.\n",
    "\n",
    "electra = AutoModel.from_pretrained(MODEL_NAME, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraConfig {\n",
       "  \"_name_or_path\": \"beomi/KcELECTRA-base\",\n",
       "  \"architectures\": [\n",
       "    \"ElectraForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"embedding_size\": 768,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"electra\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"summary_activation\": \"gelu\",\n",
       "  \"summary_last_dropout\": 0.1,\n",
       "  \"summary_type\": \"first\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50135\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we will use the default arguments, except for the last gelu for classification.\n",
    "\n",
    "electra.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader with pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOTEDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, train_dataset, test_dataset, val_dataset, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=6, ## choose a befitting number depending on your environment.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 ## about 28 ~ 30 Gb memory required, if my memory serves me right.\n",
    "\n",
    "data_module = KOTEDataModule(\n",
    "  train_dataset,\n",
    "  test_dataset,\n",
    "  val_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KOTETagger(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None, gamma_for_expLR=None):\n",
    "        super().__init__()\n",
    "        self.electra = electra\n",
    "        self.classifier = nn.Linear(self.electra.config.hidden_size, 44) ## the label dimension == 44 <-- what an ominous number for asians though... <-- I didn't intend it!\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        \n",
    "        ## the loss\n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.electra(input_ids, attention_mask=attention_mask)\n",
    "        output = output.last_hidden_state[:,0,:] ## [CLS] of the last hidden state\n",
    "        output = self.classifier(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return loss, output\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
    "\n",
    "        preds = outputs\n",
    "\n",
    "        y_true = list(labels.detach().cpu())\n",
    "        y_pred = list(preds.detach().cpu())\n",
    "\n",
    "        return {\"loss\": loss, \"y_true\": y_true, \"y_pred\": y_pred}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx)\n",
    "    \n",
    "    def epoch_end(self, outputs, state=\"train\"):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        for out in outputs:\n",
    "            loss += out[\"loss\"].detach().cpu()\n",
    "        loss = loss / len(outputs)\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for out in outputs:\n",
    "            y_true += out[\"y_true\"]\n",
    "            y_pred += out[\"y_pred\"]\n",
    "\n",
    "        self.log(state + \"_loss\", float(loss), on_epoch=True, prog_bar=True)\n",
    "        print(f\"[Epoch {self.trainer.current_epoch} {state.upper()}] Loss: {loss}\")\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state=\"train\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.epoch_end(outputs, state=\"val\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=INITIAL_LR)\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        \n",
    "        return dict(\n",
    "          optimizer=optimizer,\n",
    "          lr_scheduler=dict(\n",
    "            scheduler=scheduler,\n",
    "            interval=\"step\"\n",
    "          )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 12500)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## determine the schedule for our optimizer\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "steps_per_epoch = len(train_dataset) // BATCH_SIZE\n",
    "TOTAL_STEPS = steps_per_epoch * N_EPOCHS\n",
    "WARMUP_STEPS = TOTAL_STEPS // 5\n",
    "WARMUP_STEPS, TOTAL_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model.\n",
    "\n",
    "model = KOTETagger(\n",
    "    n_warmup_steps=WARMUP_STEPS,\n",
    "    n_training_steps=TOTAL_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set a logger and some stuffs...\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n"
    "\n",
    "## the check point\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    ###dirpath=\"YOUR DIRECTORY PATH\",\n",
    "    filename=\"epoch{epoch}-val_loss{val_loss:.4f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    "    auto_insert_metric_name=False,\n",
    ")\n",
    "\n",
    "## for early stopping\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=5, min_delta=0.00)\n",
    "\n",
    "## the logger\n",
    "logger = TensorBoardLogger(\"YOUR_FOLDER_NAME\", name=\"ONE_MORE_FOLDER_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "## trainer\n",
    "\n",
    "N_EPOCHS = 15 ## redefine the number of the epochs, just to make sure there is no more room to improve.\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, TQDMProgressBar(refresh_rate=5)],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=[2], ## GPU number\n"
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/ubuntu/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type         | Params\n",
      "--------------------------------------------\n",
      "0 | electra    | ElectraModel | 123 M \n",
      "1 | classifier | Linear       | 33.8 K\n",
      "2 | criterion  | BCELoss      | 0     \n",
      "--------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "495.953   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319caaed06bf47c081d19b7e50cea5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 VAL] Loss: 0.7097086906433105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d82c6ca1da48309adaee6ddb0ce090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 VAL] Loss: 0.3745824098587036\n",
      "[Epoch 0 TRAIN] Loss: 0.4992274045944214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 VAL] Loss: 0.3168054521083832\n",
      "[Epoch 1 TRAIN] Loss: 0.34891679883003235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 VAL] Loss: 0.2815024256706238\n",
      "[Epoch 3 TRAIN] Loss: 0.2872730791568756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 VAL] Loss: 0.27795523405075073\n",
      "[Epoch 4 TRAIN] Loss: 0.27514782547950745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5 VAL] Loss: 0.27747949957847595\n",
      "[Epoch 5 TRAIN] Loss: 0.2677864134311676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6 VAL] Loss: 0.27661705017089844\n",
      "[Epoch 6 TRAIN] Loss: 0.261890709400177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7 VAL] Loss: 0.2767089307308197\n",
      "[Epoch 7 TRAIN] Loss: 0.25691649317741394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8 VAL] Loss: 0.2769763767719269\n",
      "[Epoch 8 TRAIN] Loss: 0.25327250361442566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 9 TRAIN] Loss: 0.2511317729949951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 10 TRAIN] Loss: 0.2502129375934601\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 11 TRAIN] Loss: 0.25066524744033813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 12 TRAIN] Loss: 0.25033998489379883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 13 TRAIN] Loss: 0.2501673996448517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14 VAL] Loss: 0.2765798568725586\n",
      "[Epoch 14 TRAIN] Loss: 0.25027579069137573\n"
     ]
    }
   ],
   "source": [
    "## about 4 ~ 5 hours to reach the optimum...\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./YOUR_FOLDER_NAME/ONE_MORE_FOLDER_NAME/version_0/checkpoints/epoch9-val_loss0.2766.ckpt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "par_dir = './YOUR_FOLDER_NAME/ONE_MORE_FOLDER_NAME/version_0/checkpoints/'\n",
    "best_ckpt = sorted(glob(par_dir + '*.ckpt'))[-1]\n",
    "best_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruesome_mind_reader = KOTETagger.load_from_checkpoint(best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gruesome_mind_reader.eval()\n",
    "gruesome_mind_reader.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환영/호의: 0.8828433752059937\n",
      "감동/감탄: 0.975392758846283\n",
      "고마움: 0.320080429315567\n",
      "존경: 0.7944819927215576\n",
      "기대감: 0.722480297088623\n",
      "뿌듯함: 0.4538567066192627\n",
      "신기함/관심: 0.8110296726226807\n",
      "아껴주는: 0.8170523643493652\n",
      "즐거움/신남: 0.7045897841453552\n",
      "흐뭇함(귀여움/예쁨): 0.6821712255477905\n",
      "놀람: 0.4573516249656677\n",
      "행복: 0.6389814019203186\n",
      "기쁨: 0.7407119870185852\n",
      "안심/신뢰: 0.5029057860374451\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.3\n",
    "\n",
    "sample_text = \"고니요? 제가 아는 타짜 중에 최고였어요...\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_text,\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "_, predictions = gruesome_mind_reader(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "predictions = predictions.flatten().numpy()\n",
    "for l,p in zip(LABELS, predictions):\n",
    "    if p < THRESHOLD:\n",
    "        continue\n",
    "    print(f\"{l}: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff5ee0a5f6343d4b1c8f5e171df715d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## test\n",
    "\n",
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\") ## set the GPU number!\n",
    "gruesome_mind_reader = gruesome_mind_reader.to(DEVICE)\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(test_dataset):\n",
    "    _, pred = gruesome_mind_reader(\n",
    "        item[\"input_ids\"].unsqueeze(dim=0).to(DEVICE),\n",
    "        item[\"attention_mask\"].unsqueeze(dim=0).to(DEVICE)\n",
    "        )\n",
    "    predictions.append(pred.flatten())\n",
    "    labels.append(item[\"labels\"].round().int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8642)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.functional import accuracy, f1, auroc\n",
    "\n",
    "THRESHOLD = 0.3\n",
    "accuracy(predictions, labels, threshold=THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC per tag\n",
      "0:: 불평/불만: 0.9365819692611694\n",
      "1:: 환영/호의: 0.8934606909751892\n",
      "2:: 감동/감탄: 0.9294263124465942\n",
      "3:: 지긋지긋: 0.8327970504760742\n",
      "4:: 고마움: 0.9178640842437744\n",
      "5:: 슬픔: 0.9033969044685364\n",
      "6:: 화남/분노: 0.9369299411773682\n",
      "7:: 존경: 0.9155579209327698\n",
      "8:: 기대감: 0.8818700909614563\n",
      "9:: 우쭐댐/무시함: 0.8310950994491577\n",
      "10:: 안타까움/실망: 0.8795443177223206\n",
      "11:: 비장함: 0.860582172870636\n",
      "12:: 의심/불신: 0.8715171217918396\n",
      "13:: 뿌듯함: 0.8662175536155701\n",
      "14:: 편안/쾌적: 0.8772338628768921\n",
      "15:: 신기함/관심: 0.8687111139297485\n",
      "16:: 아껴주는: 0.8906732797622681\n",
      "17:: 부끄러움: 0.7513707876205444\n",
      "18:: 공포/무서움: 0.8868585228919983\n",
      "19:: 절망: 0.8480616807937622\n",
      "20:: 한심함: 0.8787931203842163\n",
      "21:: 역겨움/징그러움: 0.8980967402458191\n",
      "22:: 짜증: 0.9233975410461426\n",
      "23:: 어이없음: 0.8891122341156006\n",
      "24:: 없음: 0.8734162449836731\n",
      "25:: 패배/자기혐오: 0.8482953310012817\n",
      "26:: 귀찮음: 0.8192627429962158\n",
      "27:: 힘듦/지침: 0.8519691824913025\n",
      "28:: 즐거움/신남: 0.933516263961792\n",
      "29:: 깨달음: 0.8232670426368713\n",
      "30:: 죄책감: 0.8641752600669861\n",
      "31:: 증오/혐오: 0.9314982891082764\n",
      "32:: 흐뭇함(귀여움/예쁨): 0.9215825796127319\n",
      "33:: 당황/난처: 0.8416628837585449\n",
      "34:: 경악: 0.8427996635437012\n",
      "35:: 부담/안_내킴: 0.7902610898017883\n",
      "36:: 서러움: 0.8463318347930908\n",
      "37:: 재미없음: 0.8799682855606079\n",
      "38:: 불쌍함/연민: 0.8644571304321289\n",
      "39:: 놀람: 0.8527653217315674\n",
      "40:: 행복: 0.9214766621589661\n",
      "41:: 불안/걱정: 0.8555545806884766\n",
      "42:: 기쁨: 0.9326159954071045\n",
      "43:: 안심/신뢰: 0.8924616575241089\n",
      "\n",
      "MACRO_AVG :: 0.8762837052345276\n"
     ]
    }
   ],
   "source": [
    "## we should check the roc scores, since KOTE is imbalanced..!\n",
    "\n",
    "macro_auroc = []\n",
    "print(\"AUROC per tag\")\n",
    "for i, name in enumerate(LABELS):\n",
    "    try:\n",
    "        tag_auroc = auroc(predictions[:, i], labels[:, i], pos_label=1)\n",
    "        macro_auroc.append(tag_auroc)\n",
    "        print(f\"{i}:: {str(name)}: {tag_auroc}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print()\n",
    "print(\"MACRO_AVG :: {}\".format(np.array(macro_auroc).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       불평/불만       0.79      0.89      0.84      2113\n",
      "       환영/호의       0.55      0.82      0.66      1109\n",
      "       감동/감탄       0.67      0.86      0.76      1323\n",
      "        지긋지긋       0.47      0.57      0.51       816\n",
      "         고마움       0.56      0.71      0.62       637\n",
      "          슬픔       0.59      0.64      0.61       545\n",
      "       화남/분노       0.74      0.86      0.79      1538\n",
      "          존경       0.51      0.69      0.59       460\n",
      "         기대감       0.58      0.81      0.68      1359\n",
      "     우쭐댐/무시함       0.44      0.50      0.47       743\n",
      "     안타까움/실망       0.69      0.88      0.77      2185\n",
      "         비장함       0.47      0.46      0.46       416\n",
      "       의심/불신       0.62      0.77      0.69      1539\n",
      "         뿌듯함       0.43      0.56      0.49       602\n",
      "       편안/쾌적       0.45      0.51      0.48       458\n",
      "      신기함/관심       0.57      0.77      0.66      1346\n",
      "        아껴주는       0.56      0.70      0.63       897\n",
      "        부끄러움       0.33      0.07      0.11       306\n",
      "      공포/무서움       0.42      0.29      0.35       164\n",
      "          절망       0.48      0.44      0.46       472\n",
      "         한심함       0.64      0.80      0.71      1519\n",
      "    역겨움/징그러움       0.50      0.61      0.55       516\n",
      "          짜증       0.76      0.86      0.81      1909\n",
      "        어이없음       0.71      0.87      0.78      2055\n",
      "          없음       0.53      0.59      0.56       725\n",
      "     패배/자기혐오       0.38      0.25      0.30       208\n",
      "         귀찮음       0.41      0.22      0.29       290\n",
      "       힘듦/지침       0.50      0.47      0.48       473\n",
      "      즐거움/신남       0.68      0.85      0.76      1321\n",
      "         깨달음       0.49      0.62      0.55      1030\n",
      "         죄책감       0.00      0.00      0.00        84\n",
      "       증오/혐오       0.66      0.76      0.71       984\n",
      " 흐뭇함(귀여움/예쁨)       0.58      0.64      0.61       524\n",
      "       당황/난처       0.55      0.71      0.62      1319\n",
      "          경악       0.44      0.50      0.47       704\n",
      "     부담/안_내킴       0.41      0.32      0.36       606\n",
      "         서러움       0.41      0.33      0.37       263\n",
      "        재미없음       0.66      0.52      0.59       470\n",
      "      불쌍함/연민       0.53      0.59      0.56       685\n",
      "          놀람       0.55      0.63      0.59       922\n",
      "          행복       0.56      0.80      0.66       906\n",
      "       불안/걱정       0.53      0.64      0.58       960\n",
      "          기쁨       0.64      0.85      0.73      1205\n",
      "       안심/신뢰       0.54      0.73      0.62       945\n",
      "\n",
      "   micro avg       0.60      0.72      0.66     39651\n",
      "   macro avg       0.54      0.61      0.56     39651\n",
      "weighted avg       0.60      0.72      0.65     39651\n",
      " samples avg       0.61      0.75      0.65     39651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "upper, lower = 1, 0\n",
    "y_pred = np.where(y_pred > THRESHOLD, upper, lower)\n",
    "print(classification_report(\n",
    "  y_true,\n",
    "  y_pred,\n",
    "  target_names=LABELS,\n",
    "  zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed # ::: 5000\n",
      "MCC  :::::::::  0.588452811767147\n"
     ]
    }
   ],
   "source": [
    "## computation of some vector is impossible if it is a zero vector with zero variance. --> just turn off error signs\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef as MCC\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"error\")\n",
    "    \n",
    "    totalCorr = 0\n",
    "    totalLen = 0\n",
    "    for i in range(10_000):\n",
    "        try:\n",
    "            totalCorr += MCC(y_pred[i], y_true[i])\n",
    "            totalLen += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print('computed # ::: {}'.format(totalLen))\n",
    "print('MCC  :::::::::  {}'.format(totalCorr/totalLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
